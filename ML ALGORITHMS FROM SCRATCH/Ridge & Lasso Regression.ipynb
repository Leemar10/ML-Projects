{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "30ae3b0e-acc2-464d-bc59-4b375fd77f62",
   "metadata": {},
   "source": [
    "# Overview  \n",
    "\n",
    "**Implementing Machine and Deep Learning Algorithms from Scratch**  \n",
    "\n",
    "Welcome to the **second notebook** in this series!  \n",
    "\n",
    "In this notebook, we’ll dive into **three closely related algorithms**:  \n",
    "- **Ridge Regression (L2 Regularization)**  \n",
    "- **Lasso Regression (L1 Regularization)**  \n",
    "- **Elastic Net** (a combination of the two)  \n",
    "\n",
    "We’ll explore how each works, implement them from scratch, and understand the intuition behind their regularization techniques.  \n",
    "\n",
    "**Let’s get started!**  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc8eace6-ccb3-4cda-81e6-3920eec9abcd",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8db70665-6dfc-4c00-8865-c19318f7b8d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly.express as px"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c1bec99-3bdc-4b3d-b226-a076188b0b09",
   "metadata": {},
   "source": [
    "# Data Loading and Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "abc9255c-46c0-4c02-b716-3e8dc66321d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load the training and testing data\n",
    "train_data = pd.read_csv('train.csv')\n",
    "test_data = pd.read_csv('test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ceb5e7fa-a991-4d5d-a8da-5d71d9a3d249",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((700, 2), (300, 2))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.shape, test_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "afe18b53-67fe-4539-9272-975ff39de694",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x    0\n",
      "y    1\n",
      "dtype: int64\n",
      "x    0\n",
      "y    0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "#Checking for missing values\n",
    "print(train_data.isna().sum())\n",
    "print(test_data.isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f4e5f0a6-209e-427b-8771-5fcfded09b65",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = train_data.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "15d099e4-0724-444e-85a2-9d3aae536242",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x    0\n",
      "y    0\n",
      "dtype: int64\n",
      "x    0\n",
      "y    0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(train_data.isna().sum())\n",
    "print(test_data.isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c2a7a82b-ed5f-44b3-b3d5-66da1bf3c8bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>24.0</td>\n",
       "      <td>21.549452</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>50.0</td>\n",
       "      <td>47.464463</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>15.0</td>\n",
       "      <td>17.218656</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>38.0</td>\n",
       "      <td>36.586398</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>87.0</td>\n",
       "      <td>87.288984</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      x          y\n",
       "0  24.0  21.549452\n",
       "1  50.0  47.464463\n",
       "2  15.0  17.218656\n",
       "3  38.0  36.586398\n",
       "4  87.0  87.288984"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83939971-bd89-43a9-b62f-54710f41c7aa",
   "metadata": {},
   "source": [
    "# Data Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fd38ec1f-99b6-4658-9a71-383a761a215c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe\n",
       "    scrolling=\"no\"\n",
       "    width=\"100%\"\n",
       "    height=\"545px\"\n",
       "    src=\"iframe_figures/figure_8.html\"\n",
       "    frameborder=\"0\"\n",
       "    allowfullscreen\n",
       "></iframe>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = px.scatter(x=train_data['x'], y=train_data['y'], template='seaborn')\n",
    "fig.show(renderer='iframe')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffcbdbc1-fd0d-404a-ab99-bf8203e17b88",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b7bf0372-6488-47cb-b969-8a3288c79951",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Setting training features and labels\n",
    "X_train = train_data['x'].values\n",
    "y_train = train_data['y'].values\n",
    "\n",
    "#Setting testing features and labels\n",
    "X_test = test_data['x'].values\n",
    "y_test = test_data['y'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6b4e7576-4d96-48ea-bcf6-c19b3ba927b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((699,), (300,), (699,), (300,))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, X_test.shape, y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53d64de7-880a-42ed-8c42-eb89bed01aee",
   "metadata": {},
   "source": [
    "## Standardize the data\n",
    "\n",
    "Standardization is a preprocessing technique used in machine learning to rescale and transform the features (variables) of a dataset to have a mean of 0 and a standard deviation of 1. It is also known as \"z-score normalization\" or \"z-score scaling.\" Standardization is an essential step in the data preprocessing pipeline for various reasons:\n",
    "\n",
    "### Why Use Standardization in Machine Learning?\n",
    "\n",
    "1. **Mean Centering**: Standardization centers the data by subtracting the mean from each feature. This ensures that the transformed data has a mean of 0. Mean centering is crucial because it helps in capturing the relative variations in the data.\n",
    "\n",
    "2. **Scale Invariance**: Standardization scales the data by dividing each feature by its standard deviation. This makes the data scale-invariant, meaning that the scale of the features no longer affects the performance of many machine learning algorithms. Without standardization, features with larger scales may dominate the learning process.\n",
    "\n",
    "3. **Improved Convergence**: Many machine learning algorithms, such as gradient-based optimization algorithms (e.g., gradient descent), converge faster when the features are standardized. It reduces the potential for numerical instability and overflow/underflow issues during training.\n",
    "\n",
    "4. **Comparability**: Standardizing the features makes it easier to compare and interpret the importance of each feature. This is especially important in models like linear regression, where the coefficients represent the feature's impact on the target variable.\n",
    "\n",
    "5. **Regularization**: In regularization techniques like Ridge and Lasso regression, the regularization strength is applied uniformly to all features. Standardization ensures that the regularization term applies fairly to all features.\n",
    "\n",
    "### How to Standardize Data\n",
    "\n",
    "The standardization process involves the following steps:\n",
    "\n",
    "1. Calculate the mean ($\\mu$) and standard deviation ($\\sigma$) for each feature in the dataset.\n",
    "      $$\n",
    "      \\text{Mean ($\\mu$)} = \\frac{1}{n} \\sum_{i=1}^{n} x_i\n",
    "      $$\n",
    "      $$\n",
    "      \\text{Standard Deviation($\\sigma$)} = \\sqrt{\\frac{1}{n} \\sum_{i=1}^{n} (x_i - \\mu)^2}\n",
    "      $$\n",
    "2. For each data point (sample), subtract the mean ($\\mu$) of the feature and then divide by the standard deviation ($\\sigma$) of the feature.\n",
    "\n",
    "Mathematically, the standardized value for a feature `x` in a dataset is calculated as:\n",
    "\n",
    "$$\n",
    "\\text{Standardized value} = \\frac{x - \\mu}{\\sigma}\n",
    "$$\n",
    "\n",
    "Here, `x` is the original value of the feature, $\\mu$ is the mean of the feature, and $\\sigma$ is the standard deviation of the feature.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1613b980-109d-4ec4-abe4-d9916ed2ff5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Standardization:\n",
    "    def __init__(self):\n",
    "        self.mean = None\n",
    "        self.std = None\n",
    "\n",
    "    def fit_transform(self, X_train):\n",
    "        \"\"\"\n",
    "        Calculating the Mean and Standard Deviation using the training data.\n",
    "\n",
    "        Parameter: X_train(np.ndarray)\n",
    "        Returns: Standardized X_train\n",
    "        \"\"\"\n",
    "        self.mean = np.mean(X_train, axis=0)\n",
    "        self.std = np.std(X_train, axis=0)\n",
    "\n",
    "        X_train = (X_train - self.mean) / (self.std)\n",
    "\n",
    "        return X_train\n",
    "    def transform(self, X_test):\n",
    "        \"\"\"\n",
    "        Parameter: X_test(np.ndarray)\n",
    "        Returns: Standardized X_test\n",
    "        \"\"\"\n",
    "        X_test = (X_test - self.mean) / self.std\n",
    "\n",
    "        return X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8783fe91-8142-463f-884a-c7a989109717",
   "metadata": {},
   "outputs": [],
   "source": [
    "standardizer = Standardization()\n",
    "X_train = standardizer.fit_transform(X_train)\n",
    "X_test = standardizer.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5e38ec38-9b6c-4b07-957b-457f1d761013",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Currently our data is 1-D, but our model would expect 2-D data.\n",
    "X_train = np.expand_dims(X_train, axis=-1)\n",
    "X_test = np.expand_dims(X_test, axis=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8016c7a2-f982-4a65-adee-892349e8d6af",
   "metadata": {},
   "source": [
    "# Model Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56459adb-d0c4-4780-9e7c-d4c2cf80ae5c",
   "metadata": {},
   "source": [
    "# Lasso vs Ridge vs ElasticNet\n",
    "Regularization methods like Lasso, Ridge and Elastic Net help improve linear regression models by preventing overfitting which address multicollinearity and helps in feature selection. These techniques increase the model’s accuracy and stability. In this article we will see explanation of how each technique works and their differences.\n",
    "\n",
    "\n",
    "## Ridge Regression (L2 Regularization)\n",
    "Ridge regression is a technique used to address overfitting by adding a penalty to the model's complexity. It introduces an L2 penalty (also called L2 regularization) which is the sum of the squares of the model's coefficients. This penalty term reduces the size of large coefficients but keeps all features in the model. This prevents overfitting with correlated features.\n",
    "\n",
    "**Cost function for Ridge Regression:**\n",
    "$$RidgeLoss = \\frac{1}{2m}\\sum_{i=1}^{m}(y^{(i)} - \\hat{y}^{(i)})^2 + \\lambda\\sum_{i=1}^{n}\\beta_j^2$$\n",
    "\n",
    "### Notation  \n",
    "\n",
    "- $y^{(i)}$: Actual value for the $i$-th sample  \n",
    "- $\\hat{y}^{(i)}$: Predicted value for the $i$-th sample\n",
    "- $m$: The total number of samples\n",
    "- $\\beta_j$: Coefficient for the $j$-th feature (model parameters)  \n",
    "- $\\lambda$: Regularization parameter controlling the penalty strength  \n",
    "\n",
    "\n",
    "## Lasso Regression (L1 Regularization)\n",
    "Lasso regression addresses overfitting by adding an L1 penalty i.e sum of absolute coefficients to the model's loss function. This encourages some coefficients to become exactly zero helps in effectively removing less important features. It also helps to simplify the model by selecting only the key features.\n",
    "\n",
    "**Cost function for Lasso Regression:**\n",
    "$$LassoLoss = \\frac{1}{2m}\\sum_{i=1}^{m}(y^{(i)} - \\hat{y}^{(i)})^2 + \\lambda\\sum_{i=1}^{n}|\\beta_j|$$\n",
    "\n",
    "### Notation  \n",
    "\n",
    "- $y^{(i)}$: Actual value for the $i$-th sample  \n",
    "- $\\hat{y}^{(i)}$: Predicted value for the $i$-th sample\n",
    "- $m$: The total number of samples\n",
    "- $\\beta_j$: Coefficient for the $j$-th feature (model parameters)  \n",
    "- $\\lambda$: Regularization parameter controlling the penalty strength\n",
    "\n",
    "## Elastic Net Regression (L1 + L2 Regularization)\n",
    "Elastic Net regression combines both L1 (Lasso) and L2 (Ridge) penalties to perform feature selection, manage multicollinearity and balancing coefficient shrinkage. This works well when there are many correlated features helps in avoiding the problem where Lasso might randomly pick one and ignore others.\n",
    "\n",
    "**Cost function for Elastic Net Regression:**\n",
    "$$ElasticNetLoss = \\frac{1}{2m}\\sum_{i=1}^{m}(y^{(i)} - \\hat{y}^{(i)})^2 + \\lambda^{(1)}\\sum_{i=1}^{n}|\\beta_j| + \\lambda^{(2)}\\sum_{i=1}^{n}\\beta_j^2$$\n",
    "\n",
    "- $y^{(i)}$: Actual value for the $i$-th sample  \n",
    "- $\\hat{y}^{(i)}$: Predicted value for the $i$-th sample\n",
    "- $m$: The total number of samples  \n",
    "- $\\beta_j$: Coefficient for the $j$-th feature (model parameters)  \n",
    "- $\\lambda^{(1)}$: Regularization parameter controlling the penalty strength for Lasso(L1)\n",
    "- $\\lambda^{(2)}$: Regularization parameter controlling the penalty strength for Ridge(L2)\n",
    "\n",
    "**Note: While it’s possible to create three separate classes for Ridge, Lasso, and Elastic Net, I’ll instead implement a single Elastic Net class. By adjusting the regularization parameters, this one class can replicate pure Ridge (L2) or pure Lasso (L1) behavior as well.**\n",
    "\n",
    "\n",
    "## The Class Functions:\n",
    "\n",
    "### Forward Pass\n",
    "The forward pass is the step where we compute the predicted output for the input data using the current weights and biases. In other words, it’s the process of applying our model to the input data to generate predictions.\n",
    "\n",
    "### Cost Function:\n",
    "For this implementation, I’ll be using the Elastic Net cost function, which combines both L1 (Lasso) and L2 (Ridge) regularization terms:\n",
    "$$ElasticNetLoss = \\frac{1}{2m}\\sum_{i=1}^{m}(y^{(i)} - \\hat{y}^{(i)})^2 + \\lambda^{(1)}\\sum_{i=1}^{n}|\\beta_j| + \\lambda^{(2)}\\sum_{i=1}^{n}\\beta_j^2$$\n",
    "\n",
    "### Backward Pass (Gradient Computation)\n",
    "The backward pass computes the gradients of the cost function with respect to the weights and biases. These gradients are essential for updating the model parameters during training using gradient descent.\n",
    "\n",
    "The gradients are computed as follows:\n",
    "\n",
    "Gradient with respect to weights ($\\beta_j$):\n",
    "$$\\frac{\\partial Loss}{\\partial \\beta_j} = -\\frac{1}{m}\\sum_{i=1}^{m}X_j^{(i)}(y^{(i)} - \\hat{y}^{(i)}) + \\lambda^{(1)}sign\\beta_j\n",
    "+ 2\\lambda^{(2)}\\beta_j$$\n",
    "\n",
    "Where\n",
    "- $X_j^{(i)}$: the $j$-th feature of the $i$-th sample\n",
    "- $y^{(i)}$: Actual value for the $i$-th sample  \n",
    "- $\\hat{y}^{(i)}$: Predicted value for the $i$-th sample\n",
    "- $\\lambda^{(1)}$ and $\\lambda^{(2)}$ : The L1 and L2 penalty parameters respectively\n",
    "- The sign function is defined as:\n",
    "$$\n",
    "\\text{sign}(\\beta_j) =\n",
    "\\begin{cases}\n",
    "+1, & \\beta_j > 0 \\\\\n",
    "-1, & \\beta_j < 0 \\\\\n",
    "0, & \\beta_j = 0\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "\n",
    "Gradient with respect to intercept (b):\n",
    "$$\\frac{\\partial Loss}{\\partial b} = -\\frac{1}{m}\\sum_{i=1}^{m}(y^{(i)} - \\hat{y}^{(i)})$$\n",
    "\n",
    "- $y^{(i)}$: Actual value for the $i$-th sample  \n",
    "- $\\hat{y}^{(i)}$: Predicted value for the $i$-th sample\n",
    "\n",
    "## Training Process  \n",
    "\n",
    "The training process involves iteratively updating the weights and biases to minimize the cost function.  \n",
    "This is typically done using an optimization algorithm like **Gradient Descent**.  \n",
    "\n",
    "The update equations for the parameters are:  \n",
    "\n",
    "$$\\beta_j \\leftarrow \\beta_j - \\alpha\\frac{\\partial loss}{\\partial \\beta_j}$$\n",
    "$$b \\leftarrow b - \\alpha\\frac{\\partial loss}{\\partial b}$$\n",
    "\n",
    "Here, $\\alpha$ represents the **learning rate**, which controls the step size during parameter updates.  \n",
    "\n",
    "By repeatedly performing the **forward pass**, computing the **cost**, executing the **backward pass** (to compute gradients), and updating the parameters, the model gradually learns to make better predictions and fit the data more effectively.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f10eff43-a8ec-40a3-b456-b9502737fcec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class ElasticNet:\n",
    "    def __init__(self, l1_ratio, l2_ratio, learning_rate, convergence_tol=1e-6, iterations = 1000, plot_cost = False):\n",
    "        # Safety checks\n",
    "        assert iterations > 0, \"Iterations must be greater than 0\"\n",
    "        assert 0 <= l1_ratio <= 1, \"L1 ratio must be in the range 0-1\"\n",
    "        assert 0 <= l2_ratio <= 1, \"L2 ratio must be in the range 0-1\"\n",
    "        assert learning_rate > 0, \"Learning rate must be positive\"\n",
    "        assert convergence_tol > 0, \"Convergence tolerance must be positive\"\n",
    "        self.l1_ratio = l1_ratio\n",
    "        self.l2_ratio = l2_ratio\n",
    "        self.learning_rate = learning_rate\n",
    "        self.convergence_tol = convergence_tol\n",
    "        self.plot_cost = plot_cost\n",
    "        self.iterations = iterations\n",
    "\n",
    "        self.W = None\n",
    "        self.b = None\n",
    "\n",
    "    def initialize_parameters(self, n_features):\n",
    "        \"\"\"\n",
    "        Initialize model parameters\n",
    "\n",
    "        Parameters:\n",
    "        n_features (int): The number of features in the input data\n",
    "        \"\"\"\n",
    "        self.W = np.ones(n_features)\n",
    "        self.b = 0\n",
    "\n",
    "    def forward_pass(self, X):\n",
    "        \"\"\"\n",
    "        Compute the forward pass of the model\n",
    "\n",
    "        Parameters:\n",
    "        X (numpy.ndarray): Input data of shape(m, n_features)\n",
    "\n",
    "        Returns:\n",
    "        Predictions (numpy.ndarray): Predictions of shape(m,)\n",
    "        \"\"\"\n",
    "        return np.dot(X, self.W) + self.b\n",
    "\n",
    "    def calculate_cost(self, predictions):\n",
    "        \"\"\"\n",
    "        Compute the value of the mean squared error cost\n",
    "\n",
    "        Parameters:\n",
    "        Predictions (numpy.ndarray): Predictions of shape(m,)\n",
    "\n",
    "        Returns:\n",
    "            float: Mean squared error cost.\n",
    "        \"\"\"\n",
    "        m = len(self.y)\n",
    "        errors = self.y - predictions\n",
    "\n",
    "        cost = (1/(2*m)) * np.sum(np.square(errors)) \\\n",
    "               + self.l1_ratio * np.sum(np.abs(self.W)) \\\n",
    "               + self.l2_ratio * np.sum(np.square(self.W))\n",
    "        return cost\n",
    "\n",
    "    def backward_pass(self, predictions):\n",
    "        \"\"\"\n",
    "        Compute gradients for the model\n",
    "\n",
    "        Parameters:\n",
    "        predictions (numpy.ndarray): Predictions of shape (m,).\n",
    "\n",
    "        Updates:\n",
    "            numpy.ndarray: Gradient of W.\n",
    "            float: Gradient of b.\n",
    "        \"\"\"\n",
    "        m = len(predictions)\n",
    "        errors = self.y - predictions\n",
    "\n",
    "        sign_W = np.where(self.W > 0, 1, np.where(self.W < 0, -1, 0))\n",
    "\n",
    "        self.dW = (-1/m) * np.dot(self.X.T, errors)\\\n",
    "             + self.l1_ratio * sign_W \\\n",
    "             + 2 * self.l2_ratio * self.W\n",
    "        self.db = (-1/m) * np.sum(errors)\n",
    "        \n",
    "\n",
    "    def fit(self, X, y):\n",
    "        # Safety Checks\n",
    "        assert isinstance(X, np.ndarray), \"X must be a NumPy array\"\n",
    "        assert isinstance(y, np.ndarray), \"y must be a NumPy array\"\n",
    "        assert X.shape[0] == y.shape[0], \"X and y must have the same number of samples\"\n",
    "        \n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.initialize_parameters(X.shape[1])\n",
    "        costs = []\n",
    "\n",
    "        for i in range(self.iterations):\n",
    "            predictions = self.forward_pass(X)\n",
    "            costs.append(self.calculate_cost(predictions))\n",
    "            self.backward_pass(predictions)\n",
    "\n",
    "            self.W -= self.learning_rate * self.dW\n",
    "            self.b -= self.learning_rate * self.db\n",
    "\n",
    "            if i % 100 == 0:\n",
    "                print(f'Iteration: {i}, Cost: {costs[-1]}')\n",
    "\n",
    "            if i > 0 and abs(costs[-1] - costs[-2]) < self.convergence_tol:\n",
    "                print(f'Converged after {i} iterations.')\n",
    "                break\n",
    "\n",
    "        if(self.plot_cost):\n",
    "            fig = px.line(y=costs, title=\"Cost vs Iteration\", template=\"plotly_dark\")\n",
    "            fig.update_layout(\n",
    "                title_font_color=\"#41BEE9\",\n",
    "                xaxis=dict(color=\"#41BEE9\", title=\"Iterations\"),\n",
    "                yaxis=dict(color=\"#41BEE9\", title=\"Cost\")\n",
    "            )\n",
    "            fig.show(renderer='iframe')\n",
    "\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Predict target values for new input data.\n",
    "\n",
    "        Parameters:\n",
    "            X (numpy.ndarray): Input data of shape (m, n_features).\n",
    "\n",
    "        Returns:\n",
    "            numpy.ndarray: Predicted target values of shape (m,).\n",
    "        \"\"\"\n",
    "        return self.forward_pass(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1fe97094-67e9-42db-9873-86bf5b7e5bff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 0, Cost: 1642.6095800555427\n",
      "Iteration: 100, Cost: 248.1238460597492\n",
      "Iteration: 200, Cost: 61.29110082492663\n",
      "Iteration: 300, Cost: 36.25931036554461\n",
      "Iteration: 400, Cost: 32.90555921868395\n",
      "Iteration: 500, Cost: 32.45622473047303\n",
      "Iteration: 600, Cost: 32.39602304184007\n",
      "Iteration: 700, Cost: 32.38795723917112\n",
      "Iteration: 800, Cost: 32.38687658555207\n",
      "Converged after 861 iterations.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<iframe\n",
       "    scrolling=\"no\"\n",
       "    width=\"100%\"\n",
       "    height=\"545px\"\n",
       "    src=\"iframe_figures/figure_15.html\"\n",
       "    frameborder=\"0\"\n",
       "    allowfullscreen\n",
       "></iframe>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "elastic_net = ElasticNet(l1_ratio=1, l2_ratio=0, learning_rate=0.01, plot_cost=True)\n",
    "elastic_net.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa62bee5-a54d-4627-aa7f-856ad7a73555",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ff8d9c6-e104-46b6-88f8-da20d4704ae3",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "de529edb-9d7d-4ed8-a5f5-559200284afe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error, r2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "212bd9ca-5135-4b83-b283-995dc714fb2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using the model to make predictions\n",
    "y_pred = elastic_net.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e06e8167-a4e4-426f-89d5-e72dff7f08ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 11.207972373216705\n",
      "RMSE: 3.347830995318716\n",
      "R²: 0.9866941443104955\n",
      "Adjusted R²: 0.9866494937880476\n"
     ]
    }
   ],
   "source": [
    "mse_value = mean_squared_error(y_test, y_pred)\n",
    "rmse_value = np.sqrt(mse_value)\n",
    "r_squared_value = r2_score(y_test, y_pred)\n",
    "\n",
    "n = X_test.shape[0]  # number of samples\n",
    "k = X_test.shape[1]  # number of features\n",
    "\n",
    "adj_r_squared_value = 1 - (1 - r_squared_value) * (n - 1) / (n - k - 1)\n",
    "\n",
    "print(\"MSE:\", mse_value)\n",
    "print(\"RMSE:\", rmse_value)\n",
    "print(\"R²:\", r_squared_value)\n",
    "print(\"Adjusted R²:\", adj_r_squared_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaa489c5-3a21-4111-93d6-b1c0d542aaa5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
